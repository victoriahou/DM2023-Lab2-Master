{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 1 Data Preperation\n",
    "## 1.1 read data to panda_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "data_identification = pd.read_csv(\"data./data_identification.csv\", skiprows=1, header=None, names=[\"tweet_id\", \"identification\"])\n",
    "emotion = pd.read_csv(\"data./emotion.csv\", skiprows=1, header=None, names=[\"tweet_id\", \"emotion\"])\n",
    "\n",
    "tweets_DM_json=[]\n",
    "for line in open('data./tweets_DM.json', 'r'):\n",
    "    tweets_DM_json.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_tweets = [(tw['_source']['tweet']['tweet_id'],tw['_source']['tweet']['text'],tw['_source']['tweet']['hashtags'],tw['_score']) for tw in tweets_DM_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x316b80</td>\n",
       "      <td>When you buy the last 2 tickets remaining for ...</td>\n",
       "      <td>[mixedfeeling, butimTHATperson]</td>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x29d0cb</td>\n",
       "      <td>I swear all this hard work gone pay off one da...</td>\n",
       "      <td>[]</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x2a6a4f</td>\n",
       "      <td>@Parcel2Go no card left when I wasn't in so I ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "4        0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "...           ...                                                ...   \n",
       "1867530  0x316b80  When you buy the last 2 tickets remaining for ...   \n",
       "1867531  0x29d0cb  I swear all this hard work gone pay off one da...   \n",
       "1867532  0x2a6a4f  @Parcel2Go no card left when I wasn't in so I ...   \n",
       "1867533  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1867534  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                                 hashtag  _score  \n",
       "0                             [Snapchat]     391  \n",
       "1          [freepress, TrumpLegacy, CNN]     433  \n",
       "2                           [bibleverse]     232  \n",
       "3                                     []     376  \n",
       "4                                     []     989  \n",
       "...                                  ...     ...  \n",
       "1867530  [mixedfeeling, butimTHATperson]     827  \n",
       "1867531                               []     368  \n",
       "1867532                               []     498  \n",
       "1867533                               []     840  \n",
       "1867534                    [Sundayvibes]     360  \n",
       "\n",
       "[1867535 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_PD = pd.DataFrame(extract_tweets, columns=['tweet_id', 'text', 'hashtag','_score'])\n",
    "tweets_PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_identification_df = data_identification.loc[data_identification[\"identification\"] == 'train']\n",
    "train_data_identification_list = list(train_data_identification_df['tweet_id'])\n",
    "test_data_identification_df = data_identification.loc[data_identification[\"identification\"] == 'test']\n",
    "test_data_identification_list = list(test_data_identification_df['tweet_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.merge(tweets_PD, emotion, on='tweet_id')\n",
    "ORI_train_dataset = pd.merge(tweets_PD, emotion, on='tweet_id')\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>[]</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>[womendrivers]</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>@cineworld ‚Äúonly the brave‚Äù just out and fount...</td>\n",
       "      <td>[robbingmembers]</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>Felt like total dog üí© going into open gym and ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>0x2c4dc2</td>\n",
       "      <td>6 year old walks in astounded. Mum! Look how b...</td>\n",
       "      <td>[kids]</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>0x31be7c</td>\n",
       "      <td>Only one week to go until the #inspiringvolunt...</td>\n",
       "      <td>[inspiringvolunteerawards2017]</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>0x1ca58e</td>\n",
       "      <td>I just got caught up with the manga for \"My He...</td>\n",
       "      <td>[]</td>\n",
       "      <td>976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>0x35c8ba</td>\n",
       "      <td>Speak only when spoken to and make hot ass mus...</td>\n",
       "      <td>[]</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>0x1d941b</td>\n",
       "      <td>Know what you want and go for it. Fuck everyon...</td>\n",
       "      <td>[]</td>\n",
       "      <td>798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweet_id                                               text  \\\n",
       "0       0x28cc61  @Habbo I've seen two separate colours of the e...   \n",
       "1       0x2db41f  @FoxNews @KellyannePolls No serious self respe...   \n",
       "2       0x2466f6  Looking for a new car, and it says 1 lady owne...   \n",
       "3       0x23f9e9  @cineworld ‚Äúonly the brave‚Äù just out and fount...   \n",
       "4       0x1fb4e1  Felt like total dog üí© going into open gym and ...   \n",
       "...          ...                                                ...   \n",
       "411967  0x2c4dc2  6 year old walks in astounded. Mum! Look how b...   \n",
       "411968  0x31be7c  Only one week to go until the #inspiringvolunt...   \n",
       "411969  0x1ca58e  I just got caught up with the manga for \"My He...   \n",
       "411970  0x35c8ba  Speak only when spoken to and make hot ass mus...   \n",
       "411971  0x1d941b  Know what you want and go for it. Fuck everyon...   \n",
       "\n",
       "                               hashtag  _score  \n",
       "0                                   []     107  \n",
       "1                                   []     728  \n",
       "2                       [womendrivers]     491  \n",
       "3                     [robbingmembers]      28  \n",
       "4                                   []     925  \n",
       "...                                ...     ...  \n",
       "411967                          [kids]     792  \n",
       "411968  [inspiringvolunteerawards2017]      34  \n",
       "411969                              []     976  \n",
       "411970                              []     534  \n",
       "411971                              []     798  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.merge(test_data_identification_df, tweets_PD, on='tweet_id')\n",
    "test_dataset = test_dataset.drop(['identification'], axis=1)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Mis-spelling data\n",
    "\n",
    "Use some dataset on kaggle.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/datasets/bittlingmayer/spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_data = pd.read_csv(\"data/aspell.txt\", sep = \":\", names = [\"correction\",\"misspell\"])\n",
    "misspell_data\n",
    "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def misspelled_correction(val):\n",
    "    for x in val.split(): \n",
    "        if x in miss_corr.keys(): \n",
    "            val = val.replace(x, miss_corr[x]) \n",
    "    return val\n",
    "\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x : misspelled_correction(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Convert emoji to text\n",
    "Use the dictionary from this website.\n",
    "\n",
    "https://medium.com/geekculture/a-tutorial-on-performing-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk-40e5b35ab440\n",
    "\n",
    "Install emoji to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\dm_hw2\\venv\\lib\\site-packages (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_smileys():\n",
    "    return {\n",
    "        \":‚Äë)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‚ÄëD\":\"smiley\",\n",
    "        \"8‚ÄëD\":\"smiley\",\n",
    "        \"x‚ÄëD\":\"smiley\",\n",
    "        \"X‚ÄëD\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‚Äë(\":\"sad\",\n",
    "        \":‚Äëc\":\"sad\",\n",
    "        \":‚Äë<\":\"sad\",\n",
    "        \":‚Äë[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‚Äë(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‚ÄëP\":\"playful\",\n",
    "        \"X‚ÄëP\":\"playful\",\n",
    "        \"x‚Äëp\":\"playful\",\n",
    "        \":‚Äëp\":\"playful\",\n",
    "        \":‚Äë√û\":\"playful\",\n",
    "        \":‚Äë√æ\":\"playful\",\n",
    "        \":‚Äëb\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":√û\":\"playful\",\n",
    "        \":√æ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# around 30s.\n",
    "import emoji\n",
    "def remove_emoticons(tweet):\n",
    "    smilies = load_dict_smileys()\n",
    "    split_tweet = tweet.split(\" \")\n",
    "    for key,val in smilies.items():\n",
    "        if key in tweet:\n",
    "            new_tweet = tweet.replace(key, val)\n",
    "            tweet = new_tweet\n",
    "            tweet = emoji.demojize(tweet)\n",
    "    return tweet\n",
    "\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_emoticons(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Remove URLS\n",
    "Install mysmallutils to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysmallutils in c:\\dm_hw2\\venv\\lib\\site-packages (2.0.13)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pip install mysmallutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# around 10s.\n",
    "from mysutils.text import remove_urls\n",
    "URL_PATTERN = r'[A-Za-z0-9]+://[A-Za-z0-9%-_]+(/[A-Za-z0-9%-_])*(#|\\\\?)[A-Za-z0-9%-_&=]*'\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_urls(x))\n",
    "URL_PATTERN = r'[A-Za-z0-9%-_]+(/[A-Za-z0-9%-_])*(#|\\\\?)[A-Za-z0-9%-_&=]*'\n",
    "train_dataset[\"text\"] = train_dataset[\"text\"].apply(lambda x: remove_urls(x))\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Remove twitter mention\n",
    "Use pandas.dataframe.str.replace to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>As we see, Trump is dangerous to #freepress a...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>Thx for the BEST TIME tonight. What stories! ...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350   As we see, Trump is dangerous to #freepress a...   \n",
       "2        0x1cd5b0                Now ISSA is stalking Tasha üòÇüòÇüòÇ <LH>   \n",
       "3        0x1d755c   Thx for the BEST TIME tonight. What stories! ...   \n",
       "4        0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date <LH> us...   \n",
       "1455562  0x34be8c             Blessed to be living #Sundayvibes <LH>   \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\w+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*\\B@\\w+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\S+', '', regex=True)\n",
    "train_dataset['text'] = train_dataset['text'].str.replace(r'\\s*@\\S+\\b', '', regex=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Remove twitter <<LH>>\n",
    "Use pandas.dataframe.str.replace to complete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>_score</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>391</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>As we see, Trump is dangerous to #freepress a...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>433</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha üòÇüòÇüòÇ</td>\n",
       "      <td>[]</td>\n",
       "      <td>376</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>Thx for the BEST TIME tonight. What stories! ...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>120</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus.</td>\n",
       "      <td>[]</td>\n",
       "      <td>1021</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>94</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>627</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>274</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date  using ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>840</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes</td>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>360</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id                                               text  \\\n",
       "0        0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1        0x2d5350   As we see, Trump is dangerous to #freepress a...   \n",
       "2        0x1cd5b0                    Now ISSA is stalking Tasha üòÇüòÇüòÇ    \n",
       "3        0x1d755c   Thx for the BEST TIME tonight. What stories! ...   \n",
       "4        0x2c91a8           Still waiting on those supplies Liscus.    \n",
       "...           ...                                                ...   \n",
       "1455558  0x321566  I'm SO HAPPY!!! #NoWonder the name of this sho...   \n",
       "1455559  0x38959e  In every circumtance I'd like to be thankful t...   \n",
       "1455560  0x2cbca6  there's currently two girls walking around the...   \n",
       "1455561  0x24faed  Ah, corporate life, where you can date  using ...   \n",
       "1455562  0x34be8c                 Blessed to be living #Sundayvibes    \n",
       "\n",
       "                               hashtag  _score       emotion  \n",
       "0                           [Snapchat]     391  anticipation  \n",
       "1        [freepress, TrumpLegacy, CNN]     433       sadness  \n",
       "2                                   []     376          fear  \n",
       "3            [authentic, LaughOutLoud]     120           joy  \n",
       "4                                   []    1021  anticipation  \n",
       "...                                ...     ...           ...  \n",
       "1455558              [NoWonder, Happy]      94           joy  \n",
       "1455559                             []     627           joy  \n",
       "1455560                     [blessyou]     274           joy  \n",
       "1455561                             []     840           joy  \n",
       "1455562                  [Sundayvibes]     360           joy  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'] = train_dataset['text'].str.replace(r'<LH>', '', regex=True)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Remove Stopwords\n",
    "\n",
    "Use the nltk stopwords to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "train_dataset['text'] = train_dataset['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_valid = train_test_split(train_dataset, test_size=0.2)\n",
    "ORI_df_train, ORI_df_valid = train_test_split(ORI_train_dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data tokenize\n",
    "There are 3 ways to tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DM_hw2\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "BOW_500.fit(df_train['text'])\n",
    "\n",
    "X_train_BOW500 = BOW_500.transform(df_train['text'])\n",
    "X_valid_BOW500 = BOW_500.transform(df_valid['text'])\n",
    "y_train_BOW500 = df_train['emotion']\n",
    "y_valid_BOW500 = df_valid['emotion']\n",
    "\n",
    "test_bow500 = BOW_500.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DM_hw2\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "BOW_2000 = CountVectorizer(max_features=2000, tokenizer=nltk.word_tokenize)\n",
    "BOW_2000.fit(df_train['text'])\n",
    "\n",
    "X_train_BOW2000 = BOW_2000.transform(df_train['text'])\n",
    "X_valid_BOW2000 = BOW_2000.transform(df_valid['text'])\n",
    "y_train_BOW2000 = df_train['emotion']\n",
    "y_valid_BOW2000 = df_valid['emotion']\n",
    "\n",
    "test_bow2000 = BOW_2000.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "BOW_500.fit(ORI_df_train['text'])\n",
    "\n",
    "X_train_BOW500_ORI = BOW_500.transform(ORI_df_train['text'])\n",
    "X_valid_BOW500_ORI = BOW_500.transform(ORI_df_valid['text'])\n",
    "y_train_BOW500_ORI = ORI_df_train['emotion']\n",
    "y_valid_BOW500_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_bow500_ORI = BOW_500.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_2000 = CountVectorizer(max_features=2000, tokenizer=nltk.word_tokenize)\n",
    "BOW_2000.fit(ORI_df_train['text'])\n",
    "\n",
    "X_train_BOW2000_ORI = BOW_2000.transform(ORI_df_train['text'])\n",
    "X_valid_BOW2000_ORI = BOW_2000.transform(ORI_df_valid['text'])\n",
    "y_train_BOW2000_ORI = ORI_df_train['emotion']\n",
    "y_valid_BOW2000_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_bow2000_ORI = BOW_2000.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "# twt_tknzr = nltk.tokenize.TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "# tfidf_vector = TfidfVectorizer(max_features=500, tokenizer=twt_tknzr.tokenize)\n",
    "tfidf_vector = TfidfVectorizer(max_features=500) \n",
    "\n",
    "tfidf_vector.fit(ORI_df_train['text'])\n",
    "X_train_tfidf_ORI = tfidf_vector.fit_transform(ORI_df_train['text'])\n",
    "X_valid_tfidf_ORI = tfidf_vector.fit_transform(ORI_df_valid['text'])\n",
    "y_train_tfidf_ORI = ORI_df_train['emotion']\n",
    "y_valid_tfidf_ORI = ORI_df_valid['emotion']\n",
    "\n",
    "test_tfidf_tfidf_ORI = tfidf_vector.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vector = TfidfVectorizer(max_features=500) \n",
    "\n",
    "tfidf_vector.fit(ORI_df_train['text'])\n",
    "X_train_tfidf = tfidf_vector.fit_transform(df_train['text'])\n",
    "X_valid_tfidf = tfidf_vector.fit_transform(df_valid['text'])\n",
    "y_train_tfidf = df_train['emotion']\n",
    "y_valid_tfidf = df_valid['emotion']\n",
    "\n",
    "test_tfidf_tfidf = tfidf_vector.transform(test_dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(df_train['text'].tolist())\n",
    "tokenizer.fit_on_texts(df_valid['text'].tolist())\n",
    "def get_sequences(tokenizer, tweets):\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=100, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "X_train_keras = get_sequences(tokenizer, df_train['text'].tolist())\n",
    "X_valid_keras = get_sequences(tokenizer, df_valid['text'].tolist())\n",
    "\n",
    "emotions = set(df_train['emotion'].tolist())\n",
    "emotions_to_index = dict((c, i) for i, c in enumerate(emotions))\n",
    "index_to_emotions = dict((v, k) for k, v in emotions_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([emotions_to_index.get(x) for x in labels])\n",
    "\n",
    "y_train_keras = names_to_ids(df_train['emotion'].tolist())\n",
    "y_valid_keras = names_to_ids(df_valid['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=10000, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(ORI_df_train['text'].tolist())\n",
    "tokenizer.fit_on_texts(ORI_df_valid['text'].tolist())\n",
    "def get_sequences(tokenizer, tweets):\n",
    "    sequences = tokenizer.texts_to_sequences(tweets)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=100, padding='post')\n",
    "    return padded_sequences\n",
    "\n",
    "X_train_keras_ORI = get_sequences(tokenizer, ORI_df_train['text'].tolist())\n",
    "X_valid_keras_ORI = get_sequences(tokenizer, ORI_df_valid['text'].tolist())\n",
    "\n",
    "emotions = set(df_train['emotion'].tolist())\n",
    "emotions_to_index = dict((c, i) for i, c in enumerate(emotions))\n",
    "index_to_emotions = dict((v, k) for k, v in emotions_to_index.items())\n",
    "names_to_ids = lambda labels: np.array([emotions_to_index.get(x) for x in labels])\n",
    "\n",
    "y_train_keras_ORI = names_to_ids(ORI_df_train['emotion'].tolist())\n",
    "y_valid_keras_ORI = names_to_ids(ORI_df_valid['emotion'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_keras = get_sequences(tokenizer, test_dataset['text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train different models to predict\n",
    "## 3.1 DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.9017965563141397\n",
      "valid accuracy is  0.39223943966775787\n"
     ]
    }
   ],
   "source": [
    "#about 50mins\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "model = DecisionTreeClassifier(random_state=1012)\n",
    "model = model.fit(X_train_BOW500, y_train_BOW500)\n",
    "y_train_pred = model.predict(X_train_BOW500)\n",
    "y_test_pred = model.predict(X_valid_BOW500)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_BOW500, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_BOW500, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'joy', 'joy', ..., 'fear', 'anger', 'sadness'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bow500_result = model.predict(test_bow500)\n",
    "test_bow500_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_01_DT_BOW500.csv\")\n",
    "pd.DataFrame(test_bow500_result).to_csv(\"01_DT_BOW500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.3660732534673022\n",
      "valid accuracy is  0.3659438087615462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RFC = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=4, random_state=1012)\n",
    "model_RFC.fit(X_train_BOW500, y_train_BOW500)\n",
    "y_train_pred = model_RFC.predict(X_train_BOW500)\n",
    "y_test_pred = model_RFC.predict(X_valid_BOW500)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_BOW500, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_BOW500, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RFC_result = model_RFC.predict(test_bow500)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_02_RF_BOW500.csv\")\n",
    "pd.DataFrame(test_RFC_result).to_csv(\"02_RF_BOW500.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.4344815148782687\n",
      "valid accuracy is  0.3232043914218877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train_tfidf, y_train_tfidf)\n",
    "y_train_pred = MNB.predict(X_train_tfidf)\n",
    "y_test_pred = MNB.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MNB_result = MNB.predict(test_tfidf_tfidf)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_03_MNB_TFIDF500.csv\")\n",
    "pd.DataFrame(test_MNB_result).to_csv(\"03_MNB_TFIDF500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.436769290222852\n",
      "valid accuracy is  0.34386647109541657\n"
     ]
    }
   ],
   "source": [
    "MNB_tfidf = MultinomialNB()\n",
    "MNB_tfidf.fit(X_train_tfidf_ORI, y_train_tfidf_ORI)\n",
    "y_train_pred = MNB_tfidf.predict(X_train_tfidf_ORI)\n",
    "y_test_pred = MNB_tfidf.predict(X_valid_tfidf_ORI)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf_ORI, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf_ORI, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_MNB_result = MNB_tfidf.predict(test_tfidf_tfidf_ORI)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_04_MNB_TFIDF500_ORI.csv\")\n",
    "pd.DataFrame(test_MNB_result).to_csv(\"04_MNB_TFIDF500_ORI.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4  XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     -------------------------------------- 99.8/99.8 MB 168.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\dm_hw2\\venv\\lib\\site-packages (from Xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\dm_hw2\\venv\\lib\\site-packages (from Xgboost) (1.11.4)\n",
      "Installing collected packages: Xgboost\n",
      "Successfully installed Xgboost-2.0.3\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "     -------------------------------------- 99.8/99.8 MB 615.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\dm_hw2\\venv\\lib\\site-packages (from xgboost) (1.26.2)\n",
      "Requirement already satisfied: scipy in c:\\dm_hw2\\venv\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy is  0.47113572931426856\n",
      "valid accuracy is  0.0\n"
     ]
    }
   ],
   "source": [
    "# about 102mins\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_tfidf_label = le.fit_transform(y_train_tfidf)\n",
    "\n",
    "model_XGB_tfidf = XGBClassifier(n_estimators=1500, learning_rate= 0.05, max_depth=5, num_class=8)\n",
    "model_XGB_tfidf.fit(X_train_tfidf, y_train_tfidf_label)\n",
    "y_train_pred = model_XGB_tfidf.predict(X_train_tfidf)\n",
    "y_test_pred = model_XGB_tfidf.predict(X_valid_tfidf)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_train_tfidf_label, y_pred=y_train_pred)\n",
    "print(\"train accuracy is \", accuracy)\n",
    "\n",
    "accuracy = accuracy_score(y_true=y_valid_tfidf, y_pred=y_test_pred)\n",
    "print(\"valid accuracy is \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree=1000, train accuracy is 0.455, valid is unavaliable, about 102 mins.\n",
    "# tree=1500, train accuracy is 0.455, valid is unavaliable, about 130 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0 1 2 3 4 5 6 7], got ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise' 'trust']\n",
    "int_to_emotion = {'0':'anger','1':'anticipation','2':'disgust','3':'fear','4':'joy','5':'sadness','6':'surprise','7':'trust'}\n",
    "test_XGB_tfidf_result = model_XGB_tfidf.predict(test_tfidf_tfidf)\n",
    "pd.DataFrame(test_dataset.iloc[:,0]).to_csv(\"INDEX_05_XGB_tfidf.csv\")\n",
    "pd.DataFrame(test_XGB_tfidf_result).to_csv(\"05_XGB_tfidf.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "22b88c29f3c4ae317522438c45be4d1123b8810926beaba56f8ed4a7d0c6bccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
